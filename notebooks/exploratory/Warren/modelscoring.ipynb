{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn.utils import shuffle           \n",
    "import matplotlib.pyplot as plt         \n",
    "import cv2                                 \n",
    "import tensorflow as tf                \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "%matplotlib inline\n",
    "from tensorflow.keras import models, layers, regularizers, Input\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['mountain', 'street', 'glacier', 'buildings', 'sea', 'forest']\n",
    "class_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "nb_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_plots(history,name):\n",
    "    plt.style.use('ggplot')\n",
    "    \n",
    "    fig1, ax1 = plt.subplots(figsize=(6,6))\n",
    "    metrics = ['acc','val_acc']\n",
    "    for metric in metrics:\n",
    "        ax1 = plt.plot(history.history[metric], label=metric)\n",
    "    ax1 = plt.title(f'{name}-Accuracy')\n",
    "\n",
    "    ax1 = plt.legend()\n",
    "    ax1 = plt.tight_layout()\n",
    "    ax1 = plt.xlabel('Epochs')\n",
    "    ax1 = plt.ylabel('Accuracy')\n",
    "    fig1.savefig(f'Images/metrics/{name}-accuracy.jpg',bbox_inches='tight', dpi=150)\n",
    "    \n",
    "    fig2, ax2 = plt.subplots(figsize=(6,6))\n",
    "    metrics = ['loss','val_loss']\n",
    "    for metric in metrics:\n",
    "        ax2 = plt.plot(history.history[metric], label=metric)\n",
    "    ax2 = plt.title(f'{name}-Loss')\n",
    "\n",
    "    ax2 = plt.legend()\n",
    "    ax2 = plt.tight_layout()\n",
    "    ax2 = plt.xlabel('Epochs')\n",
    "    ax2 = plt.ylabel('Loss')\n",
    "    fig2.savefig(f'Images/metrics/{name}-loss.jpg',bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dims):\n",
    "    \"\"\"\n",
    "        Load the data:\n",
    "            - 14,034 images to train the network.\n",
    "            - 3,000 images to evaluate how accurately the network learned to classify images.\n",
    "    \"\"\"\n",
    "    \n",
    "    datasets = ['../../../../archive/seg_train/seg_train', '../../../../archive/seg_test/seg_test']\n",
    "    output = []\n",
    "    \n",
    "    # Iterate through training and test sets\n",
    "    for dataset in datasets:\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        print(\"Loading {}\".format(dataset))\n",
    "        \n",
    "        # Iterate through each folder corresponding to a category\n",
    "        for folder in os.listdir(dataset):\n",
    "            label = class_names_label[folder]\n",
    "            \n",
    "            # Iterate through each image in our folder\n",
    "            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n",
    "                \n",
    "                # Get the path name of the image\n",
    "                img_path = os.path.join(os.path.join(dataset, folder), file)\n",
    "                \n",
    "                # Open and resize the img\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, dims) \n",
    "                \n",
    "                # Append the image and its corresponding label to the output\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "                \n",
    "        images = np.array(images, dtype = 'float32')\n",
    "        labels = np.array(labels, dtype = 'int32')   \n",
    "        \n",
    "        output.append((images, labels))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 57/2191 [00:00<00:03, 566.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../../../archive/seg_train/seg_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2191/2191 [00:02<00:00, 758.18it/s]\n",
      "100%|██████████| 2271/2271 [00:03<00:00, 654.14it/s]\n",
      "100%|██████████| 2404/2404 [00:04<00:00, 591.64it/s]\n",
      "100%|██████████| 2512/2512 [00:04<00:00, 574.54it/s]\n",
      "100%|██████████| 2274/2274 [00:03<00:00, 588.72it/s]\n",
      "100%|██████████| 2382/2382 [00:04<00:00, 548.35it/s]\n",
      " 17%|█▋        | 75/437 [00:00<00:00, 744.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../../../archive/seg_test/seg_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 437/437 [00:00<00:00, 783.12it/s]\n",
      "100%|██████████| 474/474 [00:00<00:00, 726.57it/s]\n",
      "100%|██████████| 553/553 [00:00<00:00, 787.63it/s]\n",
      "100%|██████████| 525/525 [00:00<00:00, 780.70it/s]\n",
      "100%|██████████| 510/510 [00:00<00:00, 802.07it/s]\n",
      "100%|██████████| 501/501 [00:00<00:00, 765.55it/s]\n",
      "  8%|▊         | 168/2191 [00:00<00:01, 1664.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../../../archive/seg_train/seg_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2191/2191 [00:01<00:00, 1585.97it/s]\n",
      "100%|██████████| 2271/2271 [00:01<00:00, 1405.87it/s]\n",
      "100%|██████████| 2404/2404 [00:01<00:00, 1619.47it/s]\n",
      "100%|██████████| 2512/2512 [00:01<00:00, 1624.57it/s]\n",
      "100%|██████████| 2274/2274 [00:01<00:00, 1725.86it/s]\n",
      "100%|██████████| 2382/2382 [00:01<00:00, 1616.77it/s]\n",
      " 68%|██████▊   | 296/437 [00:00<00:00, 1377.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../../../archive/seg_test/seg_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 437/437 [00:00<00:00, 1513.07it/s]\n",
      "100%|██████████| 474/474 [00:00<00:00, 1496.03it/s]\n",
      "100%|██████████| 553/553 [00:00<00:00, 1666.49it/s]\n",
      "100%|██████████| 525/525 [00:00<00:00, 1688.09it/s]\n",
      "100%|██████████| 510/510 [00:00<00:00, 1723.86it/s]\n",
      "100%|██████████| 501/501 [00:00<00:00, 1606.69it/s]\n"
     ]
    }
   ],
   "source": [
    "(train_images_resized_75, train_labels), (test_images_resized_75, test_labels) = load_data((75,75))\n",
    "(train_images_resized_150, train_labels), (test_images_resized_150, test_labels) = load_data((150,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_resized_scaled_75 =train_images_resized_75.reshape((-1,75,75,3)).astype('float32') / 255.0\n",
    "test_images_resized_scaled_75 = test_images_resized_75.reshape((-1,75,75,3)).astype('float32') / 255.0\n",
    "\n",
    "train_images_resized_scaled_150 = train_images_resized_150.reshape((-1,150,150,3)).astype('float32') / 255.0\n",
    "test_images_resized_scaled_150 = test_images_resized_150.reshape((-1,150,150,3)).astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = '../../../../Models'\n",
    "\n",
    "def load_models(path):\n",
    "    '''Creates a tuple of filename and model\n",
    "    Takes folder path containing models'''\n",
    "    mods = []\n",
    "    #get each saved model and append it to list mods\n",
    "    for file in tqdm(os.listdir(os.path.join(path))):\n",
    "        \n",
    "        #model path\n",
    "        model_path = os.path.join(os.path.join(path), file)\n",
    "\n",
    "        mods.append((file,keras.models.load_model(model_path)))\n",
    "    return mods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "mods = load_models(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod0_hist = mods[0][1].evaluate(train_images_resized_scaled,train_labels,return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_clustered = np.load('../../../../Datasets/clusted_images_test.npz')['args']\n",
    "# train_clustered = np.load('../../../../Datasets/clusted_images_train.npz')['args']\n",
    "# test_clustered_labels = np.load('../../../../Datasets/test_labels.npz')['args']\n",
    "# train_clustered_labels = np.load('../../../../Datasets/train_labels.npz')['args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 73, 73, 512)       14336     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 24, 24, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 24, 24, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 22, 22, 156)       719004    \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 20, 20, 156)       219180    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 20, 20, 156)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 18, 18, 128)       179840    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               5308544   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 6,441,678\n",
      "Trainable params: 6,441,678\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mods[0][1].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scorer(models,train_eval_imgs_75,train_eval_imgs_150,train_eval_labels,test_eval_imgs_75,test_eval_imgs_150,test_eval_labels):\n",
    "    '''Takes a list of loaded models and evaluates them on the evaluation images and labels passed\n",
    "    returns list of tuples containing the model name and a dict of loss and accuracy for each model, can pass different sized images'''\n",
    "    model_scores = []\n",
    "    \n",
    "    for model_tup in models:\n",
    "        print(model_tup[0])\n",
    "        model_dict = {}\n",
    "        try:\n",
    "            \n",
    "            try:\n",
    "                model_dict['train'][model_tup[0].astype(str)] = model_tup[1].evaluate(train_eval_imgs_75,train_eval_labels,return_dict=True)\n",
    "                model_dict['test'][model_tup[0].astype(str)] = model_tup[1].evaluate(test_eval_imgs_75,test_eval_labels,return_dict=True)\n",
    "            \n",
    "                model_scores.append(models_dict)\n",
    "#             model_scores.append((model_tup[0],model_tup[1].evaluate(eval_imgs_75,eval_labels,return_dict=True)))\n",
    "\n",
    "            except:\n",
    "        \n",
    "                model_dict['train'][model_tup[0].astype(str)] = model_tup[1].evaluate(train_eval_imgs_150,train_eval_labels,return_dict=True)\n",
    "                model_dict['test'][model_tup[0].astype(str)] = model_tup[1].evaluate(test_eval_imgs_150,test_eval_labels,return_dict=True)\n",
    "            \n",
    "                model_scores.append(models_dict)\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            continue  \n",
    "\n",
    "#             model_scores.append((model_tup[0],model_tup[1].evaluate(eval_imgs_150,eval_labels,return_dict=True)))\n",
    "        \n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addedL1reg-withmaxpool_model.h5\n",
      "439/439 [==============================] - 115s 262ms/step - loss: 9.5569 - acc: 0.2012\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 75, 75, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 75, 75, 3), dtype=tf.float32, name='input_4'), name='input_4', description=\"created by layer 'input_4'\"), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "baseline_dropout_model.h5\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 150, 150, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name='conv2d_2_input'), name='conv2d_2_input', description=\"created by layer 'conv2d_2_input'\"), but it was called on an input with incompatible shape (None, 75, 75, 3).\n",
      "439/439 [==============================] - 36s 81ms/step - loss: 320.2752 - acc: 0.3349\n",
      "baseline_model.h5\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 150, 150, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name='conv2d_2_input'), name='conv2d_2_input', description=\"created by layer 'conv2d_2_input'\"), but it was called on an input with incompatible shape (None, 75, 75, 3).\n",
      "439/439 [==============================] - 51s 115ms/step - loss: 0.2421 - acc: 0.9273\n",
      "L2-reg_2nd-dropout_model.h5\n",
      "439/439 [==============================] - 26s 59ms/step - loss: 0.5860 - acc: 0.7963\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 75, 75, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 75, 75, 3), dtype=tf.float32, name='conv2d_5_input'), name='conv2d_5_input', description=\"created by layer 'conv2d_5_input'\"), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "L2-reg_model.h5\n",
      "439/439 [==============================] - 26s 59ms/step - loss: 0.5044 - acc: 0.8378\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 75, 75, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 75, 75, 3), dtype=tf.float32, name='conv2d_1_input'), name='conv2d_1_input', description=\"created by layer 'conv2d_1_input'\"), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "loaded_model_test_model.h5\n",
      "439/439 [==============================] - 18s 40ms/step - loss: 0.7391 - acc: 0.7257\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 75, 75, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 75, 75, 3), dtype=tf.float32, name='conv2d_2_input'), name='conv2d_2_input', description=\"created by layer 'conv2d_2_input'\"), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "more_epochs_for_lime_model.h5\n",
      "439/439 [==============================] - 26s 59ms/step - loss: 0.4317 - acc: 0.1618\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 75, 75, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 75, 75, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "more_reg_model.h5\n",
      "439/439 [==============================] - 26s 59ms/step - loss: 0.6685 - acc: 0.1341\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 75, 75, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 75, 75, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "resized_dropout_model.h5\n",
      "439/439 [==============================] - 10s 22ms/step - loss: 0.3592 - acc: 0.8773\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 75, 75, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 75, 75, 3), dtype=tf.float32, name='conv2d_5_input'), name='conv2d_5_input', description=\"created by layer 'conv2d_5_input'\"), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "resized_dropout_model_2.h5\n",
      "439/439 [==============================] - 19s 42ms/step - loss: 0.3110 - acc: 0.1953\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 75, 75, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 75, 75, 3), dtype=tf.float32, name='conv2d_input'), name='conv2d_input', description=\"created by layer 'conv2d_input'\"), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "testing_inputlayer_model.h5\n",
      "439/439 [==============================] - 26s 59ms/step - loss: 0.6500 - acc: 0.7748\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 75, 75, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 75, 75, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "test_fcn_model_2.h5\n",
      "439/439 [==============================] - 18s 40ms/step - loss: 0.2187 - acc: 0.9282\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 75, 75, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 75, 75, 3), dtype=tf.float32, name='conv2d_9_input'), name='conv2d_9_input', description=\"created by layer 'conv2d_9_input'\"), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "trimming_model.h5\n",
      "439/439 [==============================] - 18s 39ms/step - loss: 0.3753 - acc: 0.1652\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 75, 75, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 75, 75, 3), dtype=tf.float32, name='conv2d_1_input'), name='conv2d_1_input', description=\"created by layer 'conv2d_1_input'\"), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "updated_nodes_model.h5\n",
      "439/439 [==============================] - 107s 244ms/step - loss: 0.5842 - acc: 0.1553\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 75, 75, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 75, 75, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "without scaling_model.h5\n",
      "439/439 [==============================] - 26s 57ms/step - loss: 2.0542 - acc: 0.2702\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 75, 75, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 75, 75, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 150, 150, 3).\n"
     ]
    }
   ],
   "source": [
    "scores = model_scorer(mods,\n",
    "                      train_images_resized_scaled_75,train_images_resized_scaled_150,train_labels,\n",
    "                      test_images_resized_scaled_75,test_images_resized_scaled_150,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_expl(model,image_array,name,image_label):\n",
    "    '''\n",
    "    Takes a model, single image array, name string of the model, and the true label of the image.\n",
    "    Uses LIME to get an image explainer and plots the image and explaination.\n",
    "    '''\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    #instantiate explainer\n",
    "    explainer = lime_image.LimeImageExplainer(random_state=1)\n",
    "    #explain an image caste to double bc updated keras version, pass predictor \n",
    "    explanation = explainer.explain_instance(image_array.astype('double'), model.predict)\n",
    "    #show the image\n",
    "    ax = plt.imshow(image_array)\n",
    "    #get the mask for the prediction\n",
    "    temp, mask = explanation.get_image_and_mask(model.predict(image_array.reshape((1,75,75,3))).argmax(axis=1)[0], positive_only=True, hide_rest=False)\n",
    "    #show image and mask together\n",
    "    ax = plt.imshow(mark_boundaries(temp, mask))\n",
    "    \n",
    "    ax = plt.title(f'{image_label}-{name}')\n",
    "    \n",
    "\n",
    "\n",
    "    fig.savefig(f'Images/explainer/{name}-expl.jpg',bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_expl(model,train_images_resized_scaled[1500],'train-1500',train_labels[1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preds(model,image_array,name,image_label):\n",
    "    '''\n",
    "    Takes a model, single image array, name string of the model, and the true label of the image.\n",
    "    Plots the class predictions for the image.\n",
    "    '''\n",
    "    plt.style.use('fivethirtyeight')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax = plt.plot(range(0,6),model.predict(image_array.reshape((1,75,75,3)))[0])\n",
    "    ax = plt.xticks(ticks=[0,1,2,3,4,5],labels=class_names)\n",
    "    ax = plt.tight_layout()\n",
    "    ax = plt.xlabel('Class Name')\n",
    "    ax = plt.ylabel('Confidence')\n",
    "    ax = plt.title(f'{image_label}-{name}')\n",
    "    fig.savefig(f'Images/preds/{name}-preds.jpg',bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_preds(model,train_images_resized_scaled[1500],'train-1500',train_labels[1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_img = np.random.randint(0,14034,10)\n",
    "# for num in rand_img:\n",
    "#     label = train_labels[num]\n",
    "#     image_expl(model,train_images_resized_scaled[num],f'train-{num}-expl',label)\n",
    "#     image_preds(model,train_images_resized_scaled[num],f'train-{num}-pred',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
