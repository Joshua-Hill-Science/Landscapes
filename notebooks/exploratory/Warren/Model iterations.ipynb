{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn.utils import shuffle           \n",
    "import matplotlib.pyplot as plt         \n",
    "import cv2                                 \n",
    "import tensorflow as tf                \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "%matplotlib inline\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['mountain', 'street', 'glacier', 'buildings', 'sea', 'forest']\n",
    "class_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "nb_classes = len(class_names)\n",
    "\n",
    "IMAGE_SIZE = (150, 150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "        Load the data:\n",
    "            - 14,034 images to train the network.\n",
    "            - 3,000 images to evaluate how accurately the network learned to classify images.\n",
    "    \"\"\"\n",
    "    \n",
    "    datasets = ['../../../../archive/seg_train/seg_train', '../../../../archive/seg_test/seg_test']\n",
    "    output = []\n",
    "    \n",
    "    # Iterate through training and test sets\n",
    "    for dataset in datasets:\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        print(\"Loading {}\".format(dataset))\n",
    "        \n",
    "        # Iterate through each folder corresponding to a category\n",
    "        for folder in os.listdir(dataset):\n",
    "            label = class_names_label[folder]\n",
    "            \n",
    "            # Iterate through each image in our folder\n",
    "            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n",
    "                \n",
    "                # Get the path name of the image\n",
    "                img_path = os.path.join(os.path.join(dataset, folder), file)\n",
    "                \n",
    "                # Open and resize the img\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, IMAGE_SIZE) \n",
    "                \n",
    "                # Append the image and its corresponding label to the output\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "                \n",
    "        images = np.array(images, dtype = 'float32')\n",
    "        labels = np.array(labels, dtype = 'int32')   \n",
    "        \n",
    "        output.append((images, labels))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 151/2191 [00:00<00:01, 1495.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../../../archive/seg_train/seg_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2191/2191 [00:01<00:00, 1516.21it/s]\n",
      "100%|██████████| 2271/2271 [00:01<00:00, 1358.22it/s]\n",
      "100%|██████████| 2404/2404 [00:01<00:00, 1505.24it/s]\n",
      "100%|██████████| 2512/2512 [00:01<00:00, 1579.19it/s]\n",
      " 69%|██████▊   | 1558/2274 [00:00<00:00, 1646.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bdc5f874dbf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-767c0f26f4bc>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[1;31m# Open and resize the img\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                 \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_scaled = train_images.reshape((-1,150,150,3)).astype('float32') / 255.0\n",
    "test_images_scaled = test_images.reshape((-1,150,150,3)).astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_reshaped = train_images.reshape(train_images.shape[0],-1)\n",
    "test_img_reshaped = test_images.reshape(test_images.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model.add(layers.Conv2D(64,(3,3),activation='relu',input_shape=(150, 150, 3)))\n",
    "dropout_model.add(layers.MaxPooling2D(5,5))\n",
    "dropout_model.add(layers.Dropout(0.3))\n",
    "dropout_model.add(layers.Flatten())\n",
    "dropout_model.add(layers.Dense(128,activation='relu'))\n",
    "dropout_model.add(layers.Dense(6,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added checkpoints\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"../../../../Models/baseline_dropout_model.h5\", save_best_only=True)\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "                                monitor='val_loss', # What to watch\n",
    "                                min_delta=0.1, # How much change to get\n",
    "                                patience=5 # No change after 5 epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = dropout_model.fit(train_images_scaled,train_labels,\n",
    "                    batch_size=16,\n",
    "                    epochs=5,\n",
    "                    validation_data=(test_images_scaled,test_labels),\n",
    "                    callbacks=[checkpoint,earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['acc','val_acc']\n",
    "for metric in metrics:\n",
    "    plt.plot(history.history[metric], label=metric)\n",
    "plt.title('Dropout_Model-Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['loss','val_loss']\n",
    "for metric in metrics:\n",
    "    plt.plot(history.history[metric], label=metric)\n",
    "plt.title('Dropout_Model-Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try same model with resizing\n",
    "dim = (75,75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "        Load the data:\n",
    "            - 14,034 images to train the network.\n",
    "            - 3,000 images to evaluate how accurately the network learned to classify images.\n",
    "    \"\"\"\n",
    "    \n",
    "    datasets = ['../../../../archive/seg_train/seg_train', '../../../../archive/seg_test/seg_test']\n",
    "    output = []\n",
    "    \n",
    "    # Iterate through training and test sets\n",
    "    for dataset in datasets:\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        print(\"Loading {}\".format(dataset))\n",
    "        \n",
    "        # Iterate through each folder corresponding to a category\n",
    "        for folder in os.listdir(dataset):\n",
    "            label = class_names_label[folder]\n",
    "            \n",
    "            # Iterate through each image in our folder\n",
    "            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n",
    "                \n",
    "                # Get the path name of the image\n",
    "                img_path = os.path.join(os.path.join(dataset, folder), file)\n",
    "                \n",
    "                # Open and resize the img\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, dim) \n",
    "                \n",
    "                # Append the image and its corresponding label to the output\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "                \n",
    "        images = np.array(images, dtype = 'float32')\n",
    "        labels = np.array(labels, dtype = 'int32')   \n",
    "        \n",
    "        output.append((images, labels))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images_resized, train_labels), (test_images_resized, test_labels) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    import cv2\n",
    "    import IPython\n",
    "    _,ret = cv2.imencode('.jpg', img) \n",
    "    i = IPython.display.Image(data=ret)\n",
    "    IPython.display.display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before reshaping\n",
    "imshow(train_images_resized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(train_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_resized_scaled =train_images_resized.reshape((-1,75,75,3)).astype('float32') / 255.0\n",
    "test_images_resized_scaled = test_images_resized.reshape((-1,75,75,3)).astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model_resized_2 = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model_resized_2.add(layers.Conv2D(128,(3,3),activation='relu',input_shape=(75, 75, 3)))\n",
    "dropout_model_resized_2.add(layers.MaxPooling2D(3,3))\n",
    "dropout_model_resized_2.add(layers.Dropout(0.35))\n",
    "dropout_model_resized_2.add(layers.Flatten())\n",
    "dropout_model_resized_2.add(layers.Dense(128,activation='relu'))\n",
    "dropout_model_resized_2.add(layers.Dense(6,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model_resized_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model_resized_2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added checkpoints\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"../../../../Models/resized_dropout_model_2.h5\", save_best_only=True)\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "                                monitor='val_loss', # What to watch\n",
    "                                min_delta=0.1, # How much change to get\n",
    "                                patience=5 # No change after 5 epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = dropout_model_resized_2.fit(train_images_resized_scaled,train_labels,\n",
    "                    batch_size=16,\n",
    "                    epochs=5,\n",
    "                    validation_data=(test_images_resized_scaled,test_labels),\n",
    "                    callbacks=[checkpoint,earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['acc','val_acc']\n",
    "for metric in metrics:\n",
    "    plt.plot(history.history[metric], label=metric)\n",
    "plt.title('resized_Dropout_Model_2-Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['loss','val_loss']\n",
    "for metric in metrics:\n",
    "    plt.plot(history.history[metric], label=metric)\n",
    "plt.title('resized_Dropout_Model_2-Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_plots(history,name):\n",
    "    plt.style.use('ggplot')\n",
    "    \n",
    "    fig1, ax1 = plt.subplots(figsize=(6,6))\n",
    "    metrics = ['acc','val_acc']\n",
    "    for metric in metrics:\n",
    "        ax1 = plt.plot(history.history[metric], label=metric)\n",
    "    ax1 = plt.title(f'{name}-Accuracy')\n",
    "\n",
    "    ax1 = plt.legend()\n",
    "    ax1 = plt.tight_layout()\n",
    "    ax1 = plt.xlabel('Epochs')\n",
    "    ax1 = plt.ylabel('Accuracy')\n",
    "    fig1.savefig(f'Images/{name}-accuracy.jpg',bbox_inches='tight', dpi=150)\n",
    "    \n",
    "    fig2, ax2 = plt.subplots(figsize=(6,6))\n",
    "    metrics = ['loss','val_loss']\n",
    "    for metric in metrics:\n",
    "        ax2 = plt.plot(history.history[metric], label=metric)\n",
    "    ax2 = plt.title(f'{name}-Loss')\n",
    "\n",
    "    ax2 = plt.legend()\n",
    "    ax2 = plt.tight_layout()\n",
    "    ax2 = plt.xlabel('Epochs')\n",
    "    ax2 = plt.ylabel('Loss')\n",
    "    fig2.savefig(f'Images/{name}-loss.jpg',bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modeler(layers_list):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(128,(3,3),activation='relu',input_shape=(75, 75, 3)))\n",
    "\n",
    "    for layer in layers_list:\n",
    "        model.add(layer)\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128,activation='relu'))\n",
    "    model.add(layers.Dense(6,activation='softmax'))\n",
    "    \n",
    "    \n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_trainer(model,name,train_img,train_labels,test_img,test_labels,batch_size=16,epochs=5):\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "    \n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'../../../../Models/{name}_model.h5', save_best_only=True)\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "                                monitor='val_loss', # What to watch\n",
    "                                min_delta=0.1, # How much change to get\n",
    "                                patience=5 # No change after 5 epochs\n",
    "    )\n",
    "    history = model.fit(train_img,train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(test_img,test_labels),\n",
    "                    callbacks=[checkpoint,earlystop])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_list = [layers.MaxPooling2D(3,3),\n",
    "               layers.Dropout(0.35)]\n",
    "\n",
    "model = modeler(layers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_trainer(model,'test',train_images_resized_scaled,train_labels,test_images_resized_scaled,test_labels,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plots(history,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
